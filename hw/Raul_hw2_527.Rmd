---
title: "Homework 2"
author: "Raul Torres Aragon"
date: "5/2/2022"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1
In this problem, you are asked to prove some fundamental facts.  

### (a)  
Given $x_1,...,x_n, y_1,...,y_n$ and weights $w_1,...,w_n$. We are interested in solving for 
$$
\hat{\beta} \leftarrow argmin_{\beta_0,...,\beta_k} \sum_{i \leq n} w_i (y_i - (\beta_0 + \beta_1 x_i +...+\beta_kx_i^k))^2
$$  
Let $z_1 = (1, x_i, x_i^2,..., x_i^k)^T$; and $Z = (z_1, ..., z_n)^T$ be our new augmented design matrix, and $W = diag(w_1,..., w_n)$  a diagonal matrix with our weights. Show that  

$$
\hat{\beta} = (Z^TWZ)^{-1}Z^TWy
$$  

Hint: rewrite (1) in matrix/vector notation (noting that $\beta^⊤z_i = \beta_0 +β_1x_i +...+\beta_k x_i^k$ ); then write/solve the normal equations (or equivalently, take a derivative of the loss and set it equal to 0).  

Start with the loss function in matrix notation:  
$$
\text{argmin}_\beta (\boldsymbol{y} - X\boldsymbol{\beta})^T(\boldsymbol{y}-X\boldsymbol{\beta})
$$  

Now, because we want to scale each term by a $w_i$, we can then create a diagnoal matrix with weach weight on the diagnoal and the objective function becomes:  
$$
\text{argmin}_\beta (\boldsymbol{y} - X\boldsymbol{\beta})^TW(\boldsymbol{y}-X\boldsymbol{\beta})
$$  

And "expanding the (matrix) square":  

$$
\text{argmin}_\beta \bigl[\boldsymbol{y}W\boldsymbol{y}-2\boldsymbol{\beta}^TX^TW\boldsymbol{y}+\boldsymbol{\beta}^TX^TWX\boldsymbol{\beta}\bigr] = \\
\frac{\partial}{\partial\boldsymbol{\beta}}\bigl[\boldsymbol{y}W\boldsymbol{y}-2\boldsymbol{\beta}^TX^TW\boldsymbol{y}+\boldsymbol{\beta}^TX^TWX\boldsymbol{\beta}\bigr] := 0 \\
2\boldsymbol{\beta}X^TWX-2X^TW\boldsymbol{y} = 0 \\
\boldsymbol{\beta} = (X^TWX)^{-1}X^TW\boldsymbol{y}
$$  

Now, because our design matrix in this case is $Z$ as noted above, this just becomes:  
$$
\hat{\beta} = (Z^TWZ)^{-1}Z^TWy
$$  


### (b)  
Suppose we know $x_1,...,x_n ∼ N(\mu,\sigma)$ with unknown $\mu$ and $\sigma^2>0$. For  
$$
\hat{\mu} = \frac{1}{n}\sum_{i \leq n}x_i  \\
\hat{\sigma}^2 = \frac{1}{n}\sum_{i \leq n}(x_i-\hat{\mu})^2
$$


Show that for any fixed $x_0$  
$$
[\phi_{\hat{\mu},\hat{\sigma}^2}(x_0) - \phi_{\mu,\sigma^2}(x_0)]^2 = \mathcal{O}_p(\frac{1}{n})
$$

where $\phi_{\hat{\mu},\hat{\sigma}^2}(x_0)$ is our estimated gaussian density (gaussian density with plug-in estimators), and $\phi_{\mu,\sigma^2}(x_0)$ is the truth.  
Hint. Use the delta method. Also, to simplify things recall that for gaussians, $\hat{\mu}$ and $\hat{\sigma}^2$ are independent. How does this relate to what you would have gotten if you had instead used a kernel density estimate?  

Asymptotic Linear Estimators (ALE) converge in distribution to $\mathcal{N}(0, V(\phi_F))$ with $\mathcal{O}_p(\frac{1}{n})$. Okay, but what happens when our estimator cannot be computed as an empirical average? More specifically, what happens when our estimator is a function $g$ of ALEs? Assuming the $g$ is a **smooth transformation** (like a Gaussian kernel) and differentiable everywhere, then when the difference between a point $g(\boldsymbol{x})$ and nearby point $g(\boldsymbol{x}_0)$ can be well approximated through a linear function (i.e. Taylor series). This is the essence of the Delta Method, which states that if $\hat{\psi}_n$ is an ALE for parameter $\psi$ with influence function $\phi_F$ and $g:R\rightarrow R$ is differentiable at $\psi$, then $g(\hat{\psi}_n)$ is ALE of $g(\psi)$ with influence function $Z \rightarrow g'(\psi)\phi_F(z)...+\mathcal{O}_p(n^{-\frac{1}{2}})$.  

Now, $\hat{\mu}$ and $\hat{\sigma}^2$ can each be thought of as an ALE of $\mu$ and $\sigma$, and because the Gaussian kernel is a smooth function $g: R \rightarrow R$ we can apply the Multivariate Delta Method to obtain the variance of the vector-valued influence function and its convergence rate is $\mathcal{O}_p(\frac{1}{n})$.  

So, recall in multivariate Taylor approxmation is:
$$
g(\boldsymbol{a}) - g(\boldsymbol{a}_0) \approx \nabla g(\boldsymbol{a}_0)^T(\boldsymbol{a}-\boldsymbol{a}_0)
$$  
So a linearization of the problem we're trying to solve can be expressed as:  

$$
g\left(\begin{matrix} 
\hat{\mu}(x_0) \\
\hat{\sigma}^2(x_0) \\
\end{matrix}\right) - 
g\left(\begin{matrix} 
\mu(x_0) \\
\sigma^2(x_0) \\
\end{matrix}\right) = \nabla g\left(\begin{matrix} \mu(x_0) \\ \sigma(x_0) \end{matrix} \right)^T\left( \left[\begin{matrix} 
\hat{\mu(x_0)} \\
\hat{\sigma}^2(x_0) \\
\end{matrix}\right] - \left[\begin{matrix} 
\mu(x_0) \\
\sigma^2(x_0) \\
\end{matrix}\right] \right) + \mathcal{O}_p(n^{-\frac{1}{2}})
$$  

We can thus think of first term in RHS as the influence function. And we know that the expectation of the influence function squared is the variance of the (normal) distribution we get when applying the CTL. This is basically the Delta Method. Formally,  
$$
\sqrt{n}\left([g(\hat\psi_1,...,\hat\psi_n)] - [g(\psi_1,...,\psi_n)]\right) \rightarrow\mathcal{N}(0, \Sigma)  
$$  
where $\Sigma$ is 
$$
\nabla g(\psi_1,...,\psi_n)^TE\left[\boldsymbol{\phi}(x)\boldsymbol{\phi}(x)^T\right]\nabla g(\psi_1,...,g\psi_n)
$$  
where $\phi_i$ is the influence function.  

We start with our vector of ALEs,  
$$
\left[\begin{matrix}
\hat{\psi_1} \\
\hat{\psi_2}
\end{matrix}\right] = 
\left[\begin{matrix} 
\hat{\mu}(x_i) \\
\hat{\sigma}^2(x_i) \\
\end{matrix}\right] = 
\left[\begin{matrix} 
\frac{1}{n}\sum_i x_i \\
\frac{1}{n}\sum_i (x_i-\hat{\mu}) 
\end{matrix}\right]
$$  
$g$ is our Gaussian density, which is smooth and differentiable at every $x_i$.  

Thus, by the multivariate Delta Method, 
$$
\sqrt{n}\left( g( \left[\begin{matrix} 
\hat{\mu}(x_0) \\
\hat{\sigma}^2(x_0) \\
\end{matrix}\right]) - g( 
\left[\begin{matrix} 
\mu(x_0) \\
\sigma^2(x_0) \\
\end{matrix}\right] ) \right) \rightarrow^d \mathcal{N}(0, \Sigma)
$$  

which converges to zero at $\mathcal{O}_p(\frac{1}{n^2})$ and $[g_{\hat{\mu},\hat{\sigma}^2}(x_0) - g_{\mu,\sigma^2}(x_0)]^2 = \mathcal{O}_p(\frac{1}{n})$.  



## Problem 2 (skipped)  

## Problem 3  
In this question I would like you to reimplement local polynomial regression. The function ideally should take as input:  

 - A formula which includes y and x (and relates the two)  
 - A bandwidth for the kernel  
 - Optional argument — A kernel function (not a flag, an actual function)  
 - Optional argument — A vector of new x-values at which one would like predictions  
 
An example function call might look like:  

<code>local_poly(y ∼ poly(x, 2), bandwidth = 0.2, kernel = gaussian_ker_func, new_x = prespecified_new_x)</code> where <code>gaussian_ker_func</code> would be a gaussian kernel function (written by the user) and prespecified new x a new vector of $xs$ to estimate at (specified by the user).  

Note. “Optional Argument” doesn’t mean that you get to choose whether or not to implement it; it means that the user doesn’t need to pass in a value for that argument (there is a default). If no kernel function is passed in, the function should default to using the uniform kernel. If no new $x$-values are passed in for prediction, then predictions should be made at the original $x$-values.  


Two “tweaks” to make this either more interesting or easier:  

### 1. (more interesting)  
Do not use <code>lm</code> with weights to fit your model — instead solve the weighted regression problem yourself (hint. Q1 may come in handy). In this case you would likely need to use the <code>model.matrix</code> command to turn the right-hand-side of formula into a design matrix.  

```{r}
local_polyA <- function(formula, x, y, kernel = "gauss", h, x0) {
  
  # Kernel smoother
  if(kernel == "gauss"){
    w <- exp(-((x0-x)^2)/(2*h^2) )
  }else if(kernel == "epinechnikov") { 
    t <- abs(x-x0)/h
    w <- ifelse(abs(t)<=1, 3/4*(1-t^2), 0)
  }else {
    w <- iflese((x0-x)<=h, 1, 0)
  }
  
  # extract elements from the formula
  RHS <- attr(terms(formula), which = "term.labels") |> stringr::str_remove_all(" ")
  deg <- stringr::str_extract(RHS, "\\,[0-9]+\\)") |> stringr::str_extract("[0-9]+") |> as.numeric()
  
  # build Z, W, and bhat
  Z <- model.matrix(~., poly(x, degree = deg, raw = TRUE))
  W <- matrix(diag(w), ncol = length(w))
  bhat <- solve(t(Z)%*%W%*%Z) %*% t(Z)%*%W%*%y

  # prediction
  z0 <- model.matrix(~., poly(x0, degree = deg, raw = TRUE)) 
  yhat <- z0%*%bhat
  return(yhat[,1])
}
```    

### 2. (more interesting)  
Implement this so that if your kernel has bounded support, then the runtime complexity is $\mathcal{O}(nlog(n) + n^2h)$ ; as opposed to a naive implementation with runtime $\mathcal{O}(n^2)$. Note, <code>for loops</code> in R are awful, so in practice this may not speed things up (unless you write code in C/C++/julia in which case it definitely will).  

```{r}
local_polyB <- function(formula, x, y, kernel = "epinechnikov", h, x0) {
  
  # Kernel smoother
  if(kernel == "epinechnikov") { 
    t <- abs(x-x0)/h
    w <- ifelse(abs(t)<=1, 3/4*(1-t^2), 0)
  }else {
    w <- ifelse((x0-x)<=h, 1, 0)
  }
  
  # extract elements from the formula
  RHS <- attr(terms(formula), which = "term.labels") |> stringr::str_remove_all(" ")
  deg <- stringr::str_extract(RHS, "\\,[0-9]+\\)") |> stringr::str_extract("[0-9]+") |> as.numeric()
  
  # remove values outside support
  new_x <- x[w!=0]
  new_y <- y[w!=0]
  if(length(new_x) < 2) { print(paste0("too few observations inside bandwidth = ", h, ". Try a larger h."))}
  stopifnot(length(new_x)>1) 
  
  # build Z, W, and bhat
  Z <- model.matrix(~., poly(new_x, degree = deg, raw = TRUE))
  W <- matrix(diag(w[w!=0]), ncol = length(w[w!=0]))
  bhat <- solve(t(Z)%*%W%*%Z) %*% t(Z)%*%W%*%new_y
  
  # prediction
  z0 <- model.matrix(~., poly(x0, degree = deg, raw = TRUE)) 
  yhat <- z0%*%bhat
  return(yhat[,1])
}
```


### 3. (easier)  
Don’t worry about “formula notation”, or “passing in a kernel function”. Instead just pass in an x vector, a y vector, a “degree” argument (saying what degree of polynomial to use), and a “flag” (a string, or numerical variable) which indicates which kernel should be used (perhaps implement, uniform, gaussian, and epanechnikov).  

```{r}
local_polyC <- function(deg, x, y, kernel = "gauss", h, x0) {
  
  # Kernel smoother
  if(kernel == "gauss"){
    w <- exp(-((x0-x)^2)/(2*h^2) )
  }else if(kernel == "epinechnikov") { 
    t <- abs(x-x0)/h
    w <- ifelse(abs(t)<=1, 3/4*(1-t^2), 0)
  }else {
    w <- iflese((x0-x)<=h, 1, 0)
  }
  
  # build Z, W, and bhat
  Z <- model.matrix(~., poly(x, degree = deg, raw = TRUE))
  W <- matrix(diag(w), ncol = length(w))
  bhat <- solve(t(Z)%*%W%*%Z) %*% t(Z)%*%W%*%y

  # prediction
  z0 <- model.matrix(~., poly(x0, degree = deg, raw = TRUE)) 
  yhat <- z0%*%bhat
  return(yhat[,1])
}
```  

Verify the accuracy of your solver (compare to locpoly or loess) — you might get slight computational differences but should get pretty similar results.  

```{r}
n <- 1e2
x <- runif(n, -10, 10)
e <- rnorm(n, 0, 1)
y <- 2*x + 5*x^2 + e #true relationship is quadratic with coefficients = 2 and 5
x0 <- 5
```
```{r}
yhatA <- local_polyA(y~poly(x,2), x=x, y=y, h=1, x0=x0) |> round(2)
k_localpoly <- KernSmooth::locpoly(x=x, y=y, degree = 2, bandwidth = 1, gridsize = n*2, kernel = "normal")
yhat_ka <- k_localpoly$y[which.min(abs(k_localpoly$x-x0))] |> round(2)
paste0(yhatA, " vs ", yhat_ka) |> print()
```
```{r}
yhatB <- local_polyB(y~poly(x,2), x=x, y=y, h=5, x0=x0, kernel = "epinechnikov") |> round(2)
k_localpoly <- KernSmooth::locpoly(x=x, y=y, degree = 2, bandwidth = 5, gridsize = n*2, kernel = "e")
yhat_kb <- k_localpoly$y[which.min(abs(k_localpoly$x-x0))] |> round(2)
paste0(yhatB, " vs ", yhat_kb) |> print()
```
```{r}
yhatC <- local_polyC(deg=2, x=x, y=y, h=1, x0=x0, kernel = "gauss") |> round(2)
k_localpoly <- KernSmooth::locpoly(x=x, y=y, degree = 2, bandwidth = 1, gridsize = n*2, kernel = "normal")
yhat_kc <- k_localpoly$y[which.min(abs(k_localpoly$x-x0))] |> round(2)
paste0(yhatC, " vs ", yhat_kc) |> print()
```  






























